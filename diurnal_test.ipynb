{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dduncan 16/1/18\n",
    "# running some tests to examine era5 iwp variability on smaller timescales,\n",
    "#  trying to crack what's going on with day/night differences on annual timescale\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecdir = '/home/dudavid/Dendrite/Dendrite/UserAreas/Dave/EC/'\n",
    "yr = '15' # year 20x\n",
    "ts = 3 # time step in hrs of data (cant be changed--downloaded this way)\n",
    "mons = sorted(glob.glob(ecdir+yr+'*/')) # the vertical profile data are kept in separate folders\n",
    "print(len(mons))\n",
    "nm = 2 # can run 12 months but it takes up 20GB+ RAM...\n",
    "ndays = 31*nm\n",
    "nt = 8*ndays*1\n",
    "print(nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gsize = 0.5 # in degrees, normal grid (downloaded this way. if wanting to change output grid, do elsewhere)\n",
    "# downloaded at much coarser grid for comparison to DARDAR\n",
    "latmax= 90  # NS latitude limits of grid (downloaded this way)\n",
    "nx, ny = int(360.0/gsize), int(2.0*latmax/gsize)+1 \n",
    "# EC has points AT lat/lon intersections, so EC data actually -180 to 179.5, -90 to 90\n",
    "#mean_grid_iwc = np.zeros([nz, ny, nnx])\n",
    "save_swp = np.zeros([nt, ny, nx])\n",
    "count = 0 # initialize\n",
    "ndays = 0\n",
    "#iwpall = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t1=0\n",
    "for m in mons[0:nm]:\n",
    "    mo = m[len(m)-3:len(m)-1]\n",
    "    nd = int(len(sorted(os.listdir(m)))) # all daily files in the month:\n",
    "    print('starting mon: ',mo)\n",
    "    for d in range(nd):#[0:1]:\n",
    "        da = str(d+1).rjust(2,'0')\n",
    "        #for fi in range(8):\n",
    "        file = 'era5.90NS.'+da+'.'+mo+'.20'+yr+'.0.5deg.3hrly.nc'\n",
    "        f = Dataset(m+file,'r')\n",
    "        #iwc, swc = np.array(f['ciwc']), np.array(f['cswc'])  #\"specific ice/snow water content\" in kg/kg\n",
    "        swp = f['tcsw'][:]\n",
    "        swp[swp < 0 ] = 0 #np.nan\n",
    "        #print(np.shape(swp))\n",
    "        save_swp[t1:(t1+8),:,:] = swp[:,:,:] # [t,y,x]\n",
    "        t1+=8\n",
    "                \n",
    "        #avg_iw[:,:,fi] += iw[:,:]\n",
    "        #avg_sw[:,:,fi] += sw[:,:]\n",
    "        #ndays += 1 # count days processed\n",
    "print(np.shape(save_swp),np.mean(save_swp))\n",
    "save_swp = save_swp[0:t1,:,:] # trim to ndays processed\n",
    "print(np.shape(save_swp),np.mean(save_swp))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some calculations using large array:\n",
    "save_swp2 = np.zeros([ny,nx]) #init explicitly    \n",
    "pct = .99 # where to set NAN values\n",
    "lv = round(pct*t1)\n",
    "print(lv,t1-lv,t1)\n",
    "ttemp = save_swp # to not molest save_swp below?\n",
    "temp = np.sort(ttemp,axis=0)\n",
    "temp[lv:t1,:,:] = np.nan\n",
    "#print(temp[lv-20:t1,84,81])\n",
    "save_swp2 = temp\n",
    "np.shape(save_swp2)\n",
    "#print(info(save_swp))\n",
    "#print(info(save_swp2))\n",
    "        \n",
    "stddev = np.std(save_swp,axis=0)\n",
    "stddev2 = np.nanstd(save_swp2,axis=0) # now with nans for high values\n",
    "mean1 = np.mean(save_swp,axis=0) \n",
    "mean2 = np.nanmean(save_swp2,axis=0) # now with nans for high values\n",
    "print(info(stddev))\n",
    "print(info(stddev2))\n",
    "f01 = plt.figure(figsize=[14,8])\n",
    "grdmap((stddev-stddev2),-180,90,0,.5,0)\n",
    "\n",
    "std_hrly = np.zeros([8,ny,nx])\n",
    "std_hrly2 = np.zeros([8,ny,nx])\n",
    "for t in range(8):\n",
    "    std_hrly[t,:,:] = np.std(save_swp[t::8,:,:],axis=0)\n",
    "    std_hrly2[t,:,:] = np.nanstd(save_swp2[t::8,:,:],axis=0)\n",
    "    #grdmap(std_hrly[t,:,:],-180,90,.001,1)\n",
    "        \n",
    "    \n",
    "f000= plt.figure(figsize=[14,8])\n",
    "grdmap(mean1,-180,90,.001,.6,1,'mean all')\n",
    "f00 = plt.figure(figsize=[14,8])\n",
    "grdmap(mean2,-180,90,.001,.6,1,'mean '+str(100*pct)+'%')\n",
    "#print(np.shape(stddev))\n",
    "f1 = plt.figure(figsize=[14,8])\n",
    "grdmap((stddev),-180,90,.001,1,1,'sigma all')\n",
    "f10= plt.figure(figsize=[14,8])\n",
    "grdmap((stddev2),-180,90,.001,1,1,'sigma '+str(100*pct)+'%')\n",
    "f2 = plt.figure(figsize=[14,8])\n",
    "grdmap((std_hrly[0,:,:]),-180,90,.001,1,1,'hr=0')\n",
    "f3 = plt.figure(figsize=[14,8])\n",
    "grdmap((std_hrly2[0,:,:]),-180,90,.001,1,1,'hr=0 (2)')\n",
    "f4 = plt.figure(figsize=[14,8])\n",
    "grdmap((std_hrly[4,:,:]),-180,90,.001,1,1,'hr=4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f91 = plt.figure(figsize=[14,8])\n",
    "grdplt(np.flipud(stddev-stddev2),-180,180,-90,90,0,.5,0)\n",
    "f91.savefig('img/kul.jpg',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
